{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Natural Language Processing Project: Twitter Sentiment Analysis\n",
    "\n",
    "## Project Context\n",
    "\n",
    "Twitter is a form of social media with widespread use, a platform that millions of users visit to get informed on news, and also share their views. As of 2019, Twitter reported more than **330 million monthly active users**, of which 145 million do use the service daily.\n",
    "\n",
    "In this context, Twitter has been one of the protagonists of the US politics scene, as politicians of both parties, and especially President Donald Trump have used it extensively. As all sorts of opinions are shared on Tweeter (and primarily on politics, a complex topic - by default), that makes it a perfect data source for Natural Language Processing (NLP) analysis.\n",
    "\n",
    "\n",
    "## Intro\n",
    "\n",
    "For this project, I executed **sentiment analysis** on tweets about the two latest US Presidents: Donald Trump and Barack Obama. \n",
    "\n",
    "In terms of data, I collected ~4,500 tweets on them via the **Twitter API**, focusing specifically American-based tweets for this analysis.\n",
    "\n",
    "This particular notebook addresses the **data extraction**, **data preprocessing**, and its **storage into an SQL database** I am creating. The other two notebooks in the same repository tackle the tweet & sentiment analysis of the two presidents respectively.\n",
    "\n",
    "This project essentially implement an **end-to-end Data Pipeline**, from extraction to storage, and from storage to manipulation and visualiazation. \n",
    "\n",
    "In terms of tools, I have used, among others:\n",
    "- **tweepy** for data sourcing\n",
    "- **mysql** and **sqlalchemy** libraries for interaction between my Python dataframes and the MySQL database\n",
    "- **pandas** for dataframe manipulation\n",
    "- **re** library for tweet cleaning with the help of Regular Expressions\n",
    "- **geopy** for geo-mapping of tweet locations\n",
    "- **textblob** for the NLP analysis\n",
    "- **seaborn, wordcloud, plotly** for exploratory analysis\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Imports & Function definitions\n",
    "Importing the necessary libraries and modules:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import re\n",
    "import tweepy\n",
    "import pandas as pd\n",
    "import mysql.connector\n",
    "from string import punctuation\n",
    "from sqlalchemy import create_engine\n",
    "from geopy.geocoders import Nominatim\n",
    "from geopy.exc import GeocoderTimedOut"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "As part of the pre-processing, we will need to clean:\n",
    "\n",
    "a) the 'user location' of the tweets from any non-ASCII characters, such as Emojis, in order to process this info. \n",
    "\n",
    "b) the tweet text itself, by removing hashtags, punctuation, whitespaces, etc., in order to reach a clean tweet form for NLP analysis. This is done with the functions below."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# This function removes all non-ASCII characters\n",
    "def deEmojify(text): \n",
    "    if text:\n",
    "        return text.encode('ascii', 'ignore').decode('ascii')\n",
    "    else:\n",
    "        return None"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Function to clean tweets\n",
    "def processTweet(tweet):\n",
    "    # Remove HTML special entities (e.g. &amp;)\n",
    "    tweet = re.sub(r'\\&\\w*;', '', tweet)\n",
    "    # Remove tickers\n",
    "    tweet = re.sub(r'\\$\\w*', '', tweet)\n",
    "    # Convert to lowercase\n",
    "    tweet = tweet.lower()\n",
    "    # Remove hyperlinks\n",
    "    tweet = re.sub(r'https?:\\/\\/.*\\/\\w*', '', tweet)\n",
    "    # Remove hashtags\n",
    "    tweet = re.sub(r'#\\w*', '', tweet)\n",
    "    # Remove Punctuation and split 's, 't, 've with a space\n",
    "    tweet = re.sub(r'[' + punctuation.replace('@', '') + ']+', ' ', tweet)\n",
    "    # Remove words with 2 or fewer letters\n",
    "    tweet = re.sub(r'\\b\\w{1,2}\\b', '', tweet)\n",
    "    # Remove whitespace (including new line characters)\n",
    "    tweet = re.sub(r'\\s\\s+', ' ', tweet)\n",
    "    # Remove single space remaining at the front of the tweet\n",
    "    tweet = tweet.lstrip(' ') \n",
    "    # Remove any other non-ascii characters\n",
    "    tweet = tweet.encode('ascii', 'ignore').decode('ascii')\n",
    "    return tweet"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In this project, I would like to concentrate on *American-generated* tweets, as US politics is a topic primarily relevant to the country itself. Moreover, I plan to segregate the tweets sourced by **State**, and the function below will help."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Function used to geocode locations and override timeout error\n",
    "def do_geocode(address):\n",
    "    geolocator = Nominatim(user_agent=\"myTwitterApp\")\n",
    "    try:\n",
    "        return geolocator.geocode(address,exactly_one=True)\n",
    "    except GeocoderTimedOut:\n",
    "        return do_geocode(address)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Accessing the Twitter API\n",
    "\n",
    "In order to access the Twitter API and make use of tweepy stream-listening functions, one needs to apply for API credentials. There are four of them: ***consumer key, consumer secret, access token, access token secret.***\n",
    "\n",
    "After obtaining user access tokens, the code below instantiates the connection:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# twitter api credentials - needed to gain access to API\n",
    "consumer_key = \"XX\"\n",
    "consumer_secret = \"XX\"\n",
    "access_token = \"XX\"\n",
    "access_token_secret = \"XX\"\n",
    "\n",
    "# instantiating the REST API. Limitation: 18k tweets per 15 min. The flags in brackets will re-initiate it.\n",
    "auth = tweepy.OAuthHandler(consumer_key, consumer_secret)\n",
    "auth.set_access_token(access_token, access_token_secret)\n",
    "api = tweepy.API(auth,wait_on_rate_limit=True, wait_on_rate_limit_notify=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "I have decided that the ***information*** I would like to extract per tweet, is:\n",
    "- Date\n",
    "- Username\n",
    "- Screen name (how user's name appears on screen, e.g. twitter handle)\n",
    "- Tweet itself (the text)\n",
    "- Retweet Status (if it is a retweet, or an original post)\n",
    "- User location\n",
    "- Country of user's location\n",
    "- State of user's location\n",
    "- When user created his account\n",
    "- User description, as each user fills it in his/her profile\n",
    "- Number of user followers\n",
    "\n",
    "Below I create a blank dataframe to store that."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "columns = ['Date', 'Username','Screen Name', 'Text', 'Retweet Status', 'User Location','Geo Name','Country','State', 'User created at','User description','User followers']\n",
    "df = pd.DataFrame(columns= columns)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Note:** As very few (allegedly 1%) of the tweets themselves are geo-tagged by the user when posted, one cannot rely much on the tweet geotagging feature in order to infer locations of substantial tweets. \n",
    "\n",
    "Instead, I have decided to focus on the **user location** feature, of the user posting the tweet. The advantage of this, is that it allows for a richer geo-location information.\n",
    "\n",
    "A key challenge with this logic is that many users have not defined their address in a consistent way, as the input is manual. So we see ***different 'granularities'*** when it comes to 'user location', such as their actual full address, or only the city where they are based,  or only the State information.\n",
    "\n",
    "To overcome this, I will use the very convenient Geopy library, in order to map everything to the 'State' level, and subsequently, filter tweets based on this information - for consistency in the upcoming analysis."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# List of all US state names\n",
    "STATES = ['Alabama', 'Alaska', 'American Samoa', 'Arizona', 'Arkansas', 'California', 'Colorado','Connecticut', \\\n",
    "          'Delaware', 'District of Columbia', 'Federated States of Micronesia', 'Florida', 'Georgia', 'Guam', 'Hawaii',\\\n",
    "          'Idaho', 'Illinois', 'Indiana', 'Iowa', 'Kansas', 'Kentucky', 'Louisiana', 'Maine', 'Marshall Islands',\\\n",
    "          'Maryland', 'Massachusetts', 'Michigan', 'Minnesota', 'Mississippi', 'Missouri', 'Montana', 'Nebraska', \\\n",
    "          'Nevada', 'New Hampshire', 'New Jersey', 'New Mexico', 'New York', 'North Carolina', 'North Dakota', 'Northern Mariana Islands',\\\n",
    "          'Ohio', 'Oklahoma', 'Oregon', 'Palau', 'Pennsylvania', 'Puerto Rico', 'Rhode Island', 'South Carolina', 'South Dakota', \\\n",
    "          'Tennessee','Texas', 'Utah', 'Vermont', 'Virgin Islands', 'Virginia', 'Washington', 'West Virginia', 'Wisconsin', 'Wyoming']"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Retrieving and Storing the Tweets\n",
    "\n",
    "I will use the Twitter API to search for tweets that are posted in real time and contain specific words, in particular 'donald trump' and 'barack obama'. For this task, I will need:\n",
    "- a Twitter account and API credentials (as mentioned earlier)\n",
    "- a database\n",
    "- the Tweepy and mysql-connector Python libraries\n",
    "\n",
    "In terms of *Database*, I will use a **MySQL database**, as it is one of the most popular ones. For this, I have already installed the MySQL Workbench and MySQL server. In terms of Python, tweepy allows to connect to the API and stream tweets, as these are posted. Quoting the <a href=\"https://tweepy.readthedocs.io/en/v3.6.0/streaming_how_to.html?highlight=streamlistener\" target=\"_blank\"> tweepy documentation</a>, there are 3 steps to stream data:\n",
    "\n",
    "<ol>\n",
    "<li> Create a class inheriting from StreamListener </li>\n",
    "<li> Using that class create a Stream object </li>\n",
    "<li> Connect to the Twitter API using the Stream </li>\n",
    "</ol>\n",
    "\n",
    "This is done with the code below: **MystreamListener** is the class inheriting from StreamListener, and **myStream** is the object that I instantiate. Within the StreamListener class, there are pre-defined methods (init, on_status, on_error), which we will override in order to customize the behaviour. \n",
    "\n",
    "After the tweet data are retrieved, the on_status() method is called, and the following code ensures that we:\n",
    "* clean the user_location info\n",
    "* check if is referring to a US user (using the Geopy library)\n",
    "* map the user location to the correct US State, and store this info\n",
    "* check if it is a retweet or an original post\n",
    "* store all aforementioned info in a dataframe\n",
    "\n",
    "Moreover, I implement a **tweet counter** using the init() and on_status() methods, so that the console informs how many tweets we have extracted. It also prints a final message as soon as I reach a the target number of tweets containing my keyword. Then the listener returns 'False' and the stream closes the connection."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class MyStreamListener(tweepy.StreamListener):\n",
    "    \n",
    "    def __init__(self, api=None):\n",
    "        super(MyStreamListener, self).__init__()\n",
    "        self.num_tweets = 0\n",
    "    \n",
    "    def on_status(self, status):\n",
    "        \n",
    "        # Extract user location from tweets\n",
    "        user_location = deEmojify(status.user.location)\n",
    "\n",
    "        #if user_location 'is not None': If this information exists, check if it was posted from a US state\n",
    "        if user_location is not None:\n",
    "            Geocoded_location = do_geocode(user_location) \n",
    "            if (Geocoded_location is not None):\n",
    "                Geo_Name = deEmojify(Geocoded_location.raw['display_name'])\n",
    "                if (Geo_Name is not None) and ('United States of America' in Geo_Name): # here we have a US tweet\n",
    "                    Country  = 'United States of America'   # set the 'Country' as USA\n",
    "                    for i in range(len(STATES)): # iterate through our list of 50 states\n",
    "                        if STATES[i] in Geo_Name: # if the US state of the tweet was identified, store all relevant info\n",
    "                            State = STATES[i]\n",
    "                            \n",
    "                            created_at = status.created_at\n",
    "                            user_name = deEmojify(status.user.name)\n",
    "                            screen_name = deEmojify(status.user.screen_name)\n",
    "                            user_created_at = status.user.created_at\n",
    "                            user_description = deEmojify(status.user.description)\n",
    "                            user_followers_count =status.user.followers_count \n",
    "\n",
    "\n",
    "                            if hasattr(status, \"retweeted_status\"):  # Check if it was a Retweet\n",
    "                                is_retweet = '1'\n",
    "                                try:\n",
    "                                    text = status.retweeted_status.extended_tweet[\"full_text\"] # Ensure to capture its full length text - up to 280 char\n",
    "                                except AttributeError:\n",
    "                                    text = status.retweeted_status.text\n",
    "                            else:\n",
    "                                is_retweet = '0'\n",
    "                                try:\n",
    "                                    text = status.extended_tweet[\"full_text\"] # Ensure to capture its full length text - up to 280 char\n",
    "                                except AttributeError:\n",
    "                                    text = status.text\n",
    "                                \n",
    "                            text = processTweet(text)    # Pre-processing the text                            \n",
    "\n",
    "                            # adding the info to our dataframe\n",
    "                            row = [created_at, user_name, screen_name, text, is_retweet, user_location, Geo_Name, Country, State, user_created_at,\\\n",
    "                                  user_description,user_followers_count]\n",
    "                            df.loc[len(df)] = row\n",
    "        \n",
    "                            \n",
    "                             # Checking if reached the tweet target number\n",
    "                            self.num_tweets += 1\n",
    "                            print(self.num_tweets)\n",
    "                            if self.num_tweets == 2250:\n",
    "                                print(\"Reached 2500 US tweets\")\n",
    "                                return False\n",
    "                            else:\n",
    "                                return True\n",
    "\n",
    "        \n",
    "    def on_error(self, status_code):\n",
    "        \n",
    "        # Since Twitter API has rate limits, stop retrieving data when it exceeds its threshold.\n",
    "        if status_code == 420:\n",
    "            print(\"Time Error\")\n",
    "            # return False to disconnect the stream\n",
    "            return False"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Donald Trump tweets\n",
    "The code that follows extracts and stores 'Donald Trump' related tweets, on the **25th May 2020**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# call the stream looking for Donald Trump tweets\n",
    "myStreamListener = MyStreamListener()\n",
    "myStream = tweepy.Stream(auth = api.auth, listener = myStreamListener,tweet_mode='extended')\n",
    "myStream.filter(track=['trump','donald trump'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "trump_df2 = df\n",
    "trump_df2.tail()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now it is time to store the data to the MySQL database. Firstly, we create a database directly from this workbook, with the help of Python. Then, we create a Table within the database with the respective field names, and lastly, we store the data using sqlalchemy."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create a MySQL database with name TwitterTrumpDB for storing the data\n",
    "    \n",
    "mydb = mysql.connector.connect(host=\"HOSTNAME\",user=\"USERNAME\", password=\"PASSWORD\")   \n",
    "mycursor = mydb.cursor()\n",
    "mycursor.execute(\"CREATE DATABASE TwitterTrumpDB\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#now that we have created the database, let's CREATE A TABLE:\n",
    "\n",
    "mydb = mysql.connector.connect(\n",
    "    host=\"HOSTNAME\",\n",
    "    user=\"USERNAME\",\n",
    "    password=\"PASSWORD\",\n",
    "    database=\"TwitterTrumpDB\",\n",
    "    charset = 'utf8')\n",
    "\n",
    "mycursor = mydb.cursor()\n",
    "mycursor.execute(\"CREATE TABLE IF NOT EXISTS TrumpData(\\\n",
    "     created_at DATETIME,\\\n",
    "     user_name VARCHAR(255),\\\n",
    "     screen_name VARCHAR(255),\\\n",
    "     text VARCHAR(255),\\\n",
    "     is_retweet INT,\\\n",
    "     user_location VARCHAR(255),\\\n",
    "     Geo_Name VARCHAR(255),\\\n",
    "     Country VARCHAR(255),\\\n",
    "     State VARCHAR(255),\\\n",
    "     user_created_at VARCHAR(255),\\\n",
    "     user_description VARCHAR(255),\\\n",
    "     user_followers_count INT)\")\n",
    "mydb.commit()\n",
    "mycursor.close()\n",
    "mydb.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Create Engine for SQL saving\n",
    "engine = create_engine('mysql://USERNAME:PASSWORD@HOSTNAME/TwitterTrumpDB?charset=utf8')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#writing data to database\n",
    "trump_df2.to_sql('trumpdata', con=engine, if_exists='replace', index=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Barack Obama tweets\n",
    "\n",
    "We follow the exact same process for retrieving and storing tweets related to Barack Obama, split by State."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pd.DataFrame(columns= columns) # resetting the dataframe"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Instantiating the Stream for Barack Obama tweets\n",
    "myStreamListener = MyStreamListener()\n",
    "myStream = tweepy.Stream(auth = api.auth, listener = myStreamListener,tweet_mode='extended')\n",
    "myStream.filter(track=['obama','barack obama'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "obama_df2 = df\n",
    "obama_df.append(obama_df2,ignore_index = True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# CREATE a MySQL database database with the name TwitterObamaDB\n",
    "    \n",
    "mydb = mysql.connector.connect(host=\"HOSTNAME\",user=\"USERNAME\", password=\"PASSWORD\")   \n",
    "mycursor = mydb.cursor()\n",
    "mycursor.execute(\"CREATE DATABASE TwitterObamaDB\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#now that we have created the database, let's CREATE A TABLE for Obama:\n",
    "\n",
    "mydb = mysql.connector.connect(\n",
    "    host=\"HOSTNAME\",\n",
    "    user=\"USERNAME\",\n",
    "    password=\"PASSWORD\",\n",
    "    database=\"TwitterObamaDB\",\n",
    "    charset = 'utf8')\n",
    "\n",
    "mycursor = mydb.cursor()\n",
    "mycursor.execute(\"CREATE TABLE IF NOT EXISTS ObamaData(\\\n",
    "     created_at DATETIME,\\\n",
    "     user_name VARCHAR(255),\\\n",
    "     screen_name VARCHAR(255),\\\n",
    "     text VARCHAR(255),\\\n",
    "     is_retweet INT,\\\n",
    "     user_location VARCHAR(255),\\\n",
    "     Geo_Name VARCHAR(255),\\\n",
    "     Country VARCHAR(255),\\\n",
    "     State VARCHAR(255),\\\n",
    "     user_created_at VARCHAR(255),\\\n",
    "     user_description VARCHAR(255),\\\n",
    "     user_followers_count INT)\")\n",
    "mydb.commit()\n",
    "mycursor.close()\n",
    "mydb.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Create Engine for SQL saving\n",
    "engine = create_engine('mysql://USERNAME:PASSWORD@HOSTNAME/TwitterObamaDB?charset=utf8')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#writing data to database\n",
    "obama_df2.to_sql('obamadata', con=engine, if_exists='replace', index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Function to DROP A TABLE from the MySQL DB\n",
    "\n",
    "# mydb = mysql.connector.connect(\n",
    "#     host=\"XX\",\n",
    "#     user=\"XX\",\n",
    "#     password=\"XX\",\n",
    "#     database=\"XX\")\n",
    "\n",
    "# mycursor = mydb.cursor()\n",
    "# mycursor.execute(\"DROP TABLE IF EXISTS trumpData\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
