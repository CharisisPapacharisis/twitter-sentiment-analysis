{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Natural Language Processing Project: Obama Tweets\n",
    "\n",
    "## Intro\n",
    "Following up on the \"Project Intro Data Extraction\" notebook, this notebook focuses on the sentiment analysis of the Obama tweets that I extracted and stored in the MySQL database.\n",
    "\n",
    "**Hindsight**: Tweets obviously reflect trending News. The analysis that follows is based on tweets that were extracted on the **25th May 2020**. A key Twitter topic of the day was the amount of time that both presidents have spent *golfing* during their tenure, and this will be apparent in the following outputs."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "![](obama_speaking.JPG)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Imports\n",
    "Importing the necessary libraries and modules. These include -among others- sqlalchemy for **retrieving the data from MySQL**, ntlk modules and TextBlob for **NLP processing**, various **viz libaries** (e.g. seaborn, plotly, wordcloud) for different sorts of graphs."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%matplotlib inline\n",
    "\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from string import punctuation\n",
    "from textblob import TextBlob\n",
    "from collections import Counter\n",
    "import nltk\n",
    "from nltk.corpus import stopwords\n",
    "from nltk.stem.wordnet import WordNetLemmatizer\n",
    "from nltk import pos_tag\n",
    "from nltk.stem import PorterStemmer\n",
    "from nltk.tokenize import sent_tokenize\n",
    "\n",
    "from sqlalchemy import create_engine\n",
    "\n",
    "#set no limit for string printing\n",
    "pd.set_option('display.max_colwidth', -1)\n",
    "\n",
    "import seaborn as sns\n",
    "from wordcloud import WordCloud\n",
    "import matplotlib.pyplot as plt\n",
    "import plotly.graph_objects as go"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Fetching the data from the database\n",
    "Now it is time to fetch and explore the data:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Create Engine for SQL fetching\n",
    "engine = create_engine('mysql://USERNAME:PASSWORD@HOSTNAME/TwitterObamaDB?charset=utf8', echo = True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#writing data to dataframe\n",
    "obama_df = pd.read_sql_query('select * from obamadata', con= engine)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#checking the end of the dataframe\n",
    "obama_df.tail()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "By looking at the above extract, we observe a few things:\n",
    "- **User location**: as mentioned in the *'Project Intro' notebook*, users have manually specified their location with different levels of granularity, e.g. City ('Chicago'), City - State ('Dunnellon, FL'), City - State - Country ('Wynne, Arkansas, USA'). In the extraction phase, I have standardized this for consistency(columns GeoName, Country, State).\n",
    "- **Mentions**: There are user mentions (@...) which need to be removed before the analysis.\n",
    "\n",
    "Let's have a look at the dataframe composition:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "obama_df.info()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "From the above, we see that a part of the users have not specified a 'User description', but that is not an issue for our analysis. It also looks like some tweets have no text (blank), so I will make sure to filter them out. Otherwise, the data is consistent.\n",
    "\n",
    "Let's print the *tweet text* of the first tweets (**reminder**: punctuation, usernames, hashtags, etc., have been cleaned already at the stage of extraction)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Lets look at some of the tweets\n",
    "print(obama_df['Text'].head(6))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Processing\n",
    "\n",
    "Let's start by cleaning the 'twitter mentions':"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#removing the tweeter mentions @\n",
    "obama_df['Text'] = obama_df['Text'].str.replace('@[^\\s]+', '')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(obama_df['Text'].head(6))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now, I will drop the tweets with 'no text' in them. To do that, I firstly need to convert the empty strings to NaN values first, and then execute the drop."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# we convert missing strings to nan, so we can drop them\n",
    "obama_df['Text'].replace('', np.nan, inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# we drop rows where there is no text!\n",
    "obama_df.dropna(subset=['Text'], inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "obama_df.info()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "For an effective NLP analysis, we need to execute a list of actions:\n",
    "- **Remove stopwords** (e.g. 'i', 'a', 'are', 'on', 'from')\n",
    "- **Tokenization**: split tweets in smaller 'tokens', the words\n",
    "- **Lemmatization**: convert (as much as possible) these tokens to their 'canonical form', as per the <a href=\"https://en.wikipedia.org/wiki/Lemma_(morphology)\" target=\"_blank\">definition of 'Lemma'</a>\n",
    "\n",
    "This is done with the following code:\n",
    "\n",
    "### Removing Stopwords"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Firstly, we import the english stopwords\n",
    "from nltk.corpus import stopwords\n",
    "stop_words=set(stopwords.words('english'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Then, for each tweet in the dataframe, we split it in words, remove stopwords,\n",
    "# and re-join it as a text string.\n",
    "obama_df['Text'] = obama_df['Text'].apply(lambda x: ' '.join([word for word in x.split() if word not in (stop_words)]))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Tokenization & Lemmatization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "obama_df.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "WorldLemmatizer requires a **POS tag** (Part-Of-Speech tag), in order to understand if a word is e.g. a verb, noun, adjective, adverb - and process it accordingly. \n",
    "\n",
    "The below function **get_wordnet_pos** helps with that:\n",
    "\n",
    "Firstly, it gets our words as input and assigns to each a tag, thanks to the **nltk.pos_tag** tagger. The tagger uses the tagging conventions of the <a href=\"https://www.ling.upenn.edu/courses/Fall_2003/ling001/penn_treebank_pos.html\" target=\"_blank\">'Penn Treebank Project'</a>: From this dictionary, one can observe that *adjective* tags always start with J, *nouns* with NN, *verbs* with V, and *adverbs* with R.\n",
    "\n",
    "Then, the function maps the treebank tags to the wordnet corpus, a large lexical database in English - and returns a value as input for the Lemmatizer."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#TOKENIZING AND LEMMATIZING\n",
    "\n",
    "# Lemmatize with POS Tag\n",
    "from nltk.corpus import wordnet\n",
    "\n",
    "def get_wordnet_pos(word):\n",
    "    tag = nltk.pos_tag([word])[0][1][0].upper()\n",
    "#     tag_dict = {\"J\": wordnet.ADJ,\n",
    "#                 \"N\": wordnet.NOUN,\n",
    "#                 \"V\": wordnet.VERB,\n",
    "#                 \"R\": wordnet.ADV}\n",
    "\n",
    "#     return tag_dict.get(tag, wordnet.NOUN)\n",
    "    if tag.startswith('J'):\n",
    "        return wordnet.ADJ\n",
    "    elif tag.startswith('V'):\n",
    "        return wordnet.VERB\n",
    "    elif tag.startswith('N'):\n",
    "        return wordnet.NOUN\n",
    "    elif tag.startswith('R'):\n",
    "        return wordnet.ADV\n",
    "    else:\n",
    "        return wordnet.NOUN\n",
    "\n",
    "#initialize tokenizer and lemmatizer\n",
    "w_tokenizer = nltk.tokenize.WhitespaceTokenizer()\n",
    "lemmatizer = nltk.stem.WordNetLemmatizer()\n",
    "\n",
    "# apply tokenizer and lemmatizer to each tweet\n",
    "def lemmatize_text(text):\n",
    "    return [lemmatizer.lemmatize(w, get_wordnet_pos(w)) for w in w_tokenizer.tokenize(text)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Checking the lemmatized Text\n",
    "obama_df['Text']= obama_df['Text'].apply(lemmatize_text)\n",
    "obama_df.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Looking at the lemmatized results, we see an improvement towards obtaining the canonical form - e.g. in the first tweet, the word 'rising' has now been transformed to 'rise'. \n",
    "\n",
    "On top of that, I evaluated also the use of a *'stemmer'* (PorterStemmer) to simplify the words, but the results post-processing were not satisfying. Hence, I will proceed with the Lemmatizer as per above.\n",
    "\n",
    "# Data Exploration\n",
    "\n",
    "The first thing worth looking into is the frequency of words. **Which words are mostly repeated** in Barack Obama-related tweets? For this, I will plot a wordcloud, and also the frequencies per word, in descending order."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# get individual words\n",
    "words = []\n",
    "for word_list in obama_df['Text']: \n",
    "    words.extend(word_list)\n",
    "\n",
    "# create a word frequency dictionary\n",
    "wordfreq = Counter(words)\n",
    "\n",
    "#WORD CLOUD plot\n",
    "\n",
    "plt.subplots(figsize = (12,10))\n",
    "\n",
    "wordcloud = WordCloud(\n",
    "    background_color = 'white',\n",
    "    width = 1000,\n",
    "    height = 800).generate_from_frequencies(wordfreq)\n",
    "\n",
    "plt.imshow(wordcloud)\n",
    "plt.axis('off')\n",
    "plt.show() \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# print word counts of 10 top words, in descending order\n",
    "word_df = pd.DataFrame(words,columns =['names'])\n",
    "word_df['names'].value_counts().nlargest(10)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Unsurprisingly, 'obama', 'trump', 'president' are the most common words. As mentioned in the intro, 'golf' and 'golfing' have a high frequency as well, due to the topic that emerged on that day.\n",
    "\n",
    "\n",
    "## Calculating Sentiment\n",
    "\n",
    "Now it is time to **calculate the sentiment** of our tweets. The TextBlob library allows me to extract the polarity & subjectivity of tweet texts: **Polarity** comes as a float number between [-1,1], where -1 means negative, 0 means neutral, and 1 means positive. \n",
    "\n",
    "Based on this value, I create extra columns in the dataframe, marking each tweet as 'Negative', 'Neutral', 'Positive', and subsequently, I calculate the % of negative/ neutral/ positive tweets."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# create a new column (cleaned tweet), joining the words that resulted after the preprocessing, per dataframe row:\n",
    "obama_df['Clean Text'] = obama_df['Text'].apply(lambda x: ' '.join(map(str, x)))\n",
    "\n",
    "# create new dataframe columns for polarity and subjectivity\n",
    "obama_df['Polarity'] = np.nan\n",
    "obama_df['Subjectivity'] = np.nan\n",
    "obama_df['Sentiment'] = np.nan"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#reset indexing of the dataframe\n",
    "obama_df.reset_index(drop=True, inplace=True)\n",
    "\n",
    "# Create two new columns: 'Subjectivity' & 'Polarity'\n",
    "for i, text in enumerate(obama_df['Clean Text'].values): # for each row of cleaned tweets\n",
    "    #if text:  #where tweet exists\n",
    "    blob = TextBlob(text)   # assign this text to a Blob object to analyze        \n",
    "    obama_df['Subjectivity'].iloc[i] = blob.sentiment.subjectivity\n",
    "    obama_df['Polarity'].iloc[i] = blob.sentiment.polarity\n",
    "\n",
    "obama_df.loc[obama_df['Polarity'] < 0, 'Sentiment'] = 'Negative'\n",
    "obama_df.loc[obama_df['Polarity'] > 0, 'Sentiment'] = 'Positive'\n",
    "obama_df.loc[obama_df['Polarity'] == 0, 'Sentiment'] = 'Neutral'\n",
    "\n",
    "\n",
    "# Show the new dataframe with columns 'Subjectivity' & 'Polarity'\n",
    "obama_df.tail()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# counting tweets per sentiment\n",
    "obama_df['Sentiment'].value_counts()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# lets plot this as well\n",
    "groupped_sentiment = obama_df.groupby(['Sentiment'])['Text'].count().reset_index()\n",
    "#groupped_sentiment.head()\n",
    "ax =sns.barplot(x='Sentiment',y='Text',data=groupped_sentiment)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print('Percentage of positive tweets: {0:.1f}%'.format(100*len(obama_df[obama_df['Sentiment']=='Positive'])/len(obama_df)))\n",
    "print('Percentage of negative tweets: {0:.1f}%'.format(100*len(obama_df[obama_df['Sentiment']=='Negative'])/len(obama_df)))\n",
    "print('Percentage of neutral tweets: {0:.1f}%'.format(100*len(obama_df[obama_df['Sentiment']=='Neutral'])/len(obama_df)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "From the above, we see that that tweets are quite opinionated, as *more than 73% of them are either positive or negative*. As we observe, positives are actually a bit more. Below we also check the level of Polarity and Subjectivity (mean): on average, the tweets do not seem much polarized."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# mean of polarity and subjectivity is low.\n",
    "obama_df[['Polarity','Subjectivity']].describe()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Negative tweets\n",
    "We can deepdive further into the negative tweets. Let's have a look at most frequent words here."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Lets print WORDCLOUD OF ONLY NEGATIVE TWEETS\n",
    "\n",
    "# get individual words\n",
    "words2 = []\n",
    "\n",
    "for i in range(len(obama_df)):\n",
    "    #if (obama_df['Sentiment'].iloc(i)== 'Positive'):\n",
    "    if (obama_df.loc[i, 'Sentiment']=='Negative'):\n",
    "        #words.extend(text)\n",
    "        words2.extend(obama_df.loc[i, 'Text'])\n",
    "        \n",
    "        \n",
    "# create a word frequency dictionary\n",
    "wordfreq = Counter(words2)\n",
    "\n",
    "#WORD CLOUD plot\n",
    "\n",
    "plt.subplots(figsize = (12,10))\n",
    "\n",
    "wordcloud = WordCloud(\n",
    "    background_color = 'white',\n",
    "    width = 1000,\n",
    "    height = 800).generate_from_frequencies(wordfreq)\n",
    "\n",
    "plt.imshow(wordcloud)\n",
    "plt.axis('off')\n",
    "plt.show() "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# print word counts of 10 most repeated words, in descending order\n",
    "word_df2 = pd.DataFrame(words2,columns =['names'])\n",
    "word_df2['names'].value_counts().nlargest(10)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "No surprises here, with 'obama', 'golfing', 'trump' appearing in the top words. However, since we are looking only at the negative tweets now, we can also see that other words with negative connotation come up: 'outrage', 'spent'.\n",
    "\n",
    "## Positive tweets\n",
    "We can also have a look at positive tweets. Below the respective wordcloud:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Lets print WORDCLOUD OF ONLY POSITIVE TWEETS\n",
    "\n",
    "# get individual words\n",
    "words3 = []\n",
    "\n",
    "for i in range(len(obama_df)):\n",
    "    #if (obama_df['Sentiment'].iloc(i)== 'Positive'):\n",
    "    if (obama_df.loc[i, 'Sentiment']=='Positive'):\n",
    "        #words.extend(text)\n",
    "        words3.extend(obama_df.loc[i, 'Text'])\n",
    "        \n",
    "        \n",
    "# create a word frequency dictionary\n",
    "wordfreq = Counter(words3)\n",
    "\n",
    "#WORD CLOUD plot\n",
    "\n",
    "plt.subplots(figsize = (12,10))\n",
    "\n",
    "wordcloud = WordCloud(\n",
    "    background_color = 'white',\n",
    "    width = 1000,\n",
    "    height = 800).generate_from_frequencies(wordfreq)\n",
    "\n",
    "plt.imshow(wordcloud)\n",
    "plt.axis('off')\n",
    "plt.show() "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Analysis by US state\n",
    "It is very interesting to check where the negative sentiment is coming from. For this, we will group the dataframe by 'State', and aggregate at 'Sentiment' level:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "negative_groupped = obama_df[obama_df['Sentiment']=='Negative'].groupby(['State']).agg({'Sentiment':'count'}).reset_index()\n",
    "negative_groupped = negative_groupped.sort_values(by=['Sentiment'],ascending=[False])\n",
    "negative_groupped.reset_index(drop=True, inplace=True)\n",
    "\n",
    "negative_groupped.head(10)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We will do the same for positive  tweets:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "positive_groupped = obama_df[obama_df['Sentiment']=='Positive'].groupby(['State']).agg({'Sentiment':'count'}).reset_index()\n",
    "positive_groupped = positive_groupped.sort_values(by=['Sentiment'],ascending=[False])\n",
    "positive_groupped.reset_index(drop=True, inplace=True)\n",
    "\n",
    "positive_groupped.head(10)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We see that *Texas, Florida, California, and New York*, are the primary four sources of *both positive and negative tweets* (in different orders). This might also denote that these states have the most active users in Tweeter. Of course, we always need to keep in mind that our tweets sample was sourced only over one day, so it might not be as represantative.\n",
    "\n",
    "Lastly, it would be great to visualize the above in a US Map. Below comes the visualization of positive tweets for Barack Obama, colour-coded in a blue scale."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#import abbrevations\n",
    "%run US_state_dictionary.py\n",
    "\n",
    "positive_groupped['Abbr'] = positive_groupped['State'].map(us_state_dict)\n",
    "positive_groupped.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fig = go.Figure(data=go.Choropleth(\n",
    "    locations= positive_groupped['Abbr'], # Spatial coordinates\n",
    "    z = positive_groupped['Sentiment'].astype(int), # Data to be color-coded\n",
    "    locationmode = 'USA-states', # set of locations match entries in `locations`\n",
    "    colorscale = 'Blues',\n",
    "    colorbar_title = \"Count of positive tweets\",\n",
    "))\n",
    "\n",
    "fig.update_layout(\n",
    "    title_text = 'Obama Twitter Sentiment - 25 May 2020',\n",
    "    geo_scope='usa', # limite map scope to USA\n",
    ")\n",
    "\n",
    "fig.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "![](obama_graph.JPG)"
   ]
  },
  {
   "attachments": {
    "obama_figure.jpg": {
     "image/jpeg": "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"
    }
   },
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "![obama_figure.jpg](attachment:obama_figure.jpg)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
